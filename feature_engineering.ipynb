{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "3lSJTsaPiASm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering and generation"
      ]
    },
    {
      "metadata": {
        "id": "4M6fZ969cGSl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numeric Features"
      ]
    },
    {
      "metadata": {
        "id": "WAewP3sEoOsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Feature Scaling\n",
        "\n",
        "Existem modelos que dependem de Feature Scale e outros que não. Em geral, modelos lineares dependem de Feature Scale e já modelos baseados em arvores não dependem deste tipo de pré-processamento. \n",
        "\n",
        "**Nearest neighbors, modelos lineares e redes neurais performam melhor com Feature Scaling**\n",
        "\n",
        "```\n",
        "sklearn.preprocessing.MinMaxScaler\n",
        "sklearn.preprocessing.StandardScaler\n",
        "```\n",
        "\n",
        "### Outliers\n",
        "\n",
        "Os outliers são dados que estão muito fora do padrão do resto dos dados e podem fazer com que os modelos se atrapalhem durante o processo de treinamento. Uma forma de resolver esta questão é remover os itens.\n",
        "\n",
        "\n",
        "```\n",
        "upperbound, lowerbound = np.percentile(x, [1, 99])\n",
        "y = np.clip(x, upperbound, lowerbound)\n",
        "```\n",
        "\n",
        "### Rank Transformation\n",
        "\n",
        " Este tipo de transformação pode ser uma opção melhor do que MinMaxScaler se houverem Outliers, porque esta transformação irá mover os outliers para mais próximo de outros objetos. Veja o exemplo abaixo:\n",
        "\n",
        "\n",
        "*   rank([-100, 0, 1e5]) = [0, 1, 2]\n",
        "*   rank([1000,1,10]) = [2,0,1]\n",
        "\n",
        "Modelos lineares, KNN e redes neurais podem se beneficiar deste tipo de transformação se você não tiver tempo para tratar os outliers de forma manual.\n",
        "\n",
        "```\n",
        "scipy.stats.rankdata\n",
        "```\n",
        "\n",
        "### Log transform:\n",
        "Ajuda modelos não baseados em árvores e especialmente redes neurais.\n",
        "\n",
        "Este tipo de transformação faz com que features com valores muito grandes cheguem mais próximos dos valores médios. Além disso, valores mais próximos de zero se tornam mais fáceis de serem identificados. Apesar da simplicidade isso pode aumentar significativamente os resultados de **redes neurais**\n",
        "\n",
        "```\n",
        "np.log(1 + x)\n",
        "```\n",
        "\n",
        "### Raising to the power < 1:\n",
        "Ajuda modelos não baseados em árvores e especialmente redes neurais.\n",
        "\n",
        "Este tipo de transformação faz com que features com valores muito grandes cheguem mais próximos dos valores médios. Além disso, valores mais próximos de zero se tornam mais fáceis de serem identificados. Apesar da simplicidade isso pode aumentar significativamente os resultados de **redes neurais**\n",
        "\n",
        "```\n",
        "np.sqrt(x + 2/3)\n",
        "```\n",
        "\n",
        "## Ideias interessantes\n",
        "\n",
        "\n",
        "\n",
        "*   Concatenar o mesmo dataframe com diferentes técnicas de pré-processamento\n",
        "*   Unir modelos que são treinados com dados pré-processados com técnicas diferentes\n",
        "*   Estas ideias beneficiam principalmente **KNN, Linear Models e neural networks**\n",
        "\n",
        "## Conclusão\n",
        "\n",
        "\n",
        "\n",
        "1.   Processamento de features numericas são diferentes para modelos tree-based and non-tree based:\n",
        "      *   Modelos Tree-based não são afetados por feature scaling\n",
        "      *   Modelos não tree-based são **extremamente** dependentes de feature scaling\n",
        "2.   As técnicas de pré-processamento mais utilizadas são:\n",
        "      *   MinMaxScaler - to [0, 1]\n",
        "      *   StandardScaler - to mean==0, std==1\n",
        "      *   Rank - sets spaces between sorted values to be equal\n",
        "      *   np.log(1+x) and np.sqrt(1+x)"
      ]
    },
    {
      "metadata": {
        "id": "xBLbfp3njeYO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Categorical Features"
      ]
    },
    {
      "metadata": {
        "id": "ox0VEukywpuZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ordinal features\n",
        "\n",
        "Quando temos características diferentes entre as categorias, onde uma tende a ser maior ou representar algo mais complexo que a outra. Por exemplo:\n",
        "\n",
        "* Ticket class 1, 2, 3\n",
        "* Tipo da carteira de motorista: A, B, C, D\n",
        "* Jardim de infancia, escola, colegial, graduação, mestrado, doutorado e pós-doutorado.\n",
        "\n",
        "A forma mais simples é basicamente mapear as características cada uma com um número diferente. Podemos utilizar a técnica chamada **Label Encoding**\n",
        "\n",
        "#### Label Encoding\n",
        "\n",
        "Este método funciona muito bem com modelos baseados em árvore (tree-model), modelos não tree-model não utilizam este tipo de feature de forma efetiva. \n",
        "\n",
        "Podemos fazer label encoding de duas maneiras:\n",
        "\n",
        "```\n",
        "# Faz o encoding ordenado em ordem alfabetica\n",
        "# Ex.: [S, C, Q] -> [2, 1, 3]\n",
        "\n",
        "sklearn.preprocessing.LabelEncoder\n",
        "\n",
        "# Faz o label encoding pela frequencia\n",
        "# Ex.: [S, C, Q] -> [1, 2, 3]\n",
        "\n",
        "Pandas.factorize\n",
        "```\n",
        "\n",
        "**Caso queira treinar um modelo linear, KNN ou Rede neural, você precisa tratar estes dados de forma diferente. Utilizando One-hot encoding**\n",
        "\n",
        "#### One-hot-enconding\n",
        "\n",
        "```\n",
        "pandas.get_dummies\n",
        "sklearn.preprocessing.OneHotEncoder\n",
        "```\n",
        "\n",
        "Quando você possui um dataframe com muitos zeros e alguns uns você tem uma matrix esparsa (Sparce Matrices) e é importante aprender a lidar com este tipo de dado de forma eficiente para reduzir o consumo de memória.\n",
        "\n",
        "Ler mais em: [SkLearn Sparsity](http://scikit-learn.org/stable/modules/feature_extraction.html#sparsity)\n",
        "\n",
        "\n",
        "#### Frequency Encoding\n",
        "\n",
        "Podemos mapear os dados de acordo com a frequencia que eles aparecem na base de dados. Por exemplo, se um determinado valor (C) está em 30% da base, 50% do valor (S) está na base e 20% dos valores são (Q) então teremos:\n",
        "\n",
        "* [S, C, Q] -> [0.50, 0.30, 0.20]\n",
        "\n",
        "\n",
        "Este tipo de técnica vai preservar informações sobre a distribuição dos valores e pode **ajudar tanto modelos lineares quanto tree-models**\n",
        "\n",
        "```\n",
        "encoding = titanic.groupby('Embarked').size()\n",
        "encoding = encoding / len(titanic)\n",
        "titanic['enc'] = titanic.Embarked.map(enconding)\n",
        "```\n",
        "\n",
        "#### Unindo features categoricas\n",
        "\n",
        "Quando existe uma correlação entre duas ou mais features com o target, é interessante criar os dummies da união das duas features: \n",
        "Exemplo:\n",
        "\n",
        "```\n",
        "pclass = [3, 1, 3, 1]\n",
        "sex = ['male', 'female', 'female', 'female']\n",
        "\n",
        "# Basta criar uma outra feature concatenando o texto das duas features juntas\n",
        "pclass_sex = ['male3', '1female', '3female', '1female']\n",
        "```\n",
        "\n",
        "Agora executar o get_dummies e teremos a relação entre as duas categorias.\n",
        "\n",
        "\n",
        "### Conclusão:\n",
        "\n",
        "1. Values in ordinal features are sorted in some meaningful order\n",
        "2. Label encoding maps categories to numbers\n",
        "3. Frequency encoding maps categories to their frequencies\n",
        "4. label and Frequency encodings are often used for tree-based models\n",
        "5. One-hot encoding is often used for non-tree-based models\n",
        "6. Interactions of categorical features can he,help linear models and kNN\n"
      ]
    },
    {
      "metadata": {
        "id": "iIa4UbxojhhU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Krgwa6xjju5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Categorical and ordinal features"
      ]
    },
    {
      "metadata": {
        "id": "N1AKw4MXjmb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_mN8_4EjnFH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Datetime and coordinates"
      ]
    },
    {
      "metadata": {
        "id": "6uBu6SXPj-Pu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWucGosJkJq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Handling missing values"
      ]
    },
    {
      "metadata": {
        "id": "A_j1v0ErkKwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqWs4COIkYHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mean encoding"
      ]
    },
    {
      "metadata": {
        "id": "Ep1kt60_kZEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1bW05uPkZhs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regularization"
      ]
    },
    {
      "metadata": {
        "id": "UihCLN_Aka53",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1Jx-mbdkdAy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extensions and generalizations"
      ]
    },
    {
      "metadata": {
        "id": "YXmq5iZ2kfhf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O7uLqTceki6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Statistics and distance based features"
      ]
    },
    {
      "metadata": {
        "id": "XrYUbDXrklHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfcw9fW_klkM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Matrix factorizations"
      ]
    },
    {
      "metadata": {
        "id": "G0QyeNSAknV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AO_OVB-6knzD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature interactions"
      ]
    },
    {
      "metadata": {
        "id": "VjCaya3lkpB4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6d-pDR3PkpbC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## t-SNE"
      ]
    },
    {
      "metadata": {
        "id": "jQxYIRcjkqug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOYwIZttkrNq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KNN Features Implementation"
      ]
    },
    {
      "metadata": {
        "id": "WNs71VsHku2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}