{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/thiagobrito/datasciencenotes/blob/master/feature_engineering.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "3lSJTsaPiASm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering and generation"
      ]
    },
    {
      "metadata": {
        "id": "4M6fZ969cGSl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numeric Features"
      ]
    },
    {
      "metadata": {
        "id": "WAewP3sEoOsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Feature Scaling\n",
        "\n",
        "Existem modelos que dependem de Feature Scale e outros que não. Em geral, modelos lineares dependem de Feature Scale e já modelos baseados em arvores não dependem deste tipo de pré-processamento. \n",
        "\n",
        "**Nearest neighbors, modelos lineares e redes neurais performam melhor com Feature Scaling**\n",
        "\n",
        "```\n",
        "sklearn.preprocessing.MinMaxScaler\n",
        "sklearn.preprocessing.StandardScaler\n",
        "```\n",
        "\n",
        "### Outliers\n",
        "\n",
        "Os outliers são dados que estão muito fora do padrão do resto dos dados e podem fazer com que os modelos se atrapalhem durante o processo de treinamento. Uma forma de resolver esta questão é remover os itens.\n",
        "\n",
        "\n",
        "```\n",
        "upperbound, lowerbound = np.percentile(x, [1, 99])\n",
        "y = np.clip(x, upperbound, lowerbound)\n",
        "```\n",
        "\n",
        "### Rank Transformation\n",
        "\n",
        " Este tipo de transformação pode ser uma opção melhor do que MinMaxScaler se houverem Outliers, porque esta transformação irá mover os outliers para mais próximo de outros objetos. Veja o exemplo abaixo:\n",
        "\n",
        "\n",
        "*   rank([-100, 0, 1e5]) = [0, 1, 2]\n",
        "*   rank([1000,1,10]) = [2,0,1]\n",
        "\n",
        "Modelos lineares, KNN e redes neurais podem se beneficiar deste tipo de transformação se você não tiver tempo para tratar os outliers de forma manual.\n",
        "\n",
        "```\n",
        "scipy.stats.rankdata\n",
        "```\n",
        "\n",
        "### Log transform:\n",
        "Ajuda modelos não baseados em árvores e especialmente redes neurais.\n",
        "\n",
        "Este tipo de transformação faz com que features com valores muito grandes cheguem mais próximos dos valores médios. Além disso, valores mais próximos de zero se tornam mais fáceis de serem identificados. Apesar da simplicidade isso pode aumentar significativamente os resultados de **redes neurais**\n",
        "\n",
        "```\n",
        "np.log(1 + x)\n",
        "```\n",
        "\n",
        "### Raising to the power < 1:\n",
        "Ajuda modelos não baseados em árvores e especialmente redes neurais.\n",
        "\n",
        "Este tipo de transformação faz com que features com valores muito grandes cheguem mais próximos dos valores médios. Além disso, valores mais próximos de zero se tornam mais fáceis de serem identificados. Apesar da simplicidade isso pode aumentar significativamente os resultados de **redes neurais**\n",
        "\n",
        "```\n",
        "np.sqrt(x + 2/3)\n",
        "```\n",
        "\n",
        "## Ideias interessantes\n",
        "\n",
        "\n",
        "\n",
        "*   Concatenar o mesmo dataframe com diferentes técnicas de pré-processamento\n",
        "*   Unir modelos que são treinados com dados pré-processados com técnicas diferentes\n",
        "*   Estas ideias beneficiam principalmente **KNN, Linear Models e neural networks**\n",
        "\n",
        "## Conclusão\n",
        "\n",
        "\n",
        "\n",
        "1.   Processamento de features numericas são diferentes para modelos tree-based and non-tree based:\n",
        "      *   Modelos Tree-based não são afetados por feature scaling\n",
        "      *   Modelos não tree-based são **extremamente** dependentes de feature scaling\n",
        "2.   As técnicas de pré-processamento mais utilizadas são:\n",
        "      *   MinMaxScaler - to [0, 1]\n",
        "      *   StandardScaler - to mean==0, std==1\n",
        "      *   Rank - sets spaces between sorted values to be equal\n",
        "      *   np.log(1+x) and np.sqrt(1+x)"
      ]
    },
    {
      "metadata": {
        "id": "xBLbfp3njeYO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Categorical  and ordinal features"
      ]
    },
    {
      "metadata": {
        "id": "ox0VEukywpuZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ordinal features\n",
        "\n",
        "Quando temos características diferentes entre as categorias, onde uma tende a ser maior ou representar algo mais complexo que a outra. Por exemplo:\n",
        "\n",
        "* Ticket class 1, 2, 3\n",
        "* Tipo da carteira de motorista: A, B, C, D\n",
        "* Jardim de infancia, escola, colegial, graduação, mestrado, doutorado e pós-doutorado.\n",
        "\n",
        "A forma mais simples é basicamente mapear as características cada uma com um número diferente. Podemos utilizar a técnica chamada **Label Encoding**\n",
        "\n",
        "#### Label Encoding\n",
        "\n",
        "Este método funciona muito bem com modelos baseados em árvore (tree-model), modelos não tree-model não utilizam este tipo de feature de forma efetiva. \n",
        "\n",
        "Podemos fazer label encoding de duas maneiras:\n",
        "\n",
        "```\n",
        "# Faz o encoding ordenado em ordem alfabetica\n",
        "# Ex.: [S, C, Q] -> [2, 1, 3]\n",
        "\n",
        "sklearn.preprocessing.LabelEncoder\n",
        "\n",
        "# Faz o label encoding pela frequencia\n",
        "# Ex.: [S, C, Q] -> [1, 2, 3]\n",
        "\n",
        "Pandas.factorize\n",
        "```\n",
        "\n",
        "**Caso queira treinar um modelo linear, KNN ou Rede neural, você precisa tratar estes dados de forma diferente. Utilizando One-hot encoding**\n",
        "\n",
        "#### One-hot-enconding\n",
        "\n",
        "```\n",
        "pandas.get_dummies\n",
        "sklearn.preprocessing.OneHotEncoder\n",
        "```\n",
        "\n",
        "Quando você possui um dataframe com muitos zeros e alguns uns você tem uma matrix esparsa (Sparce Matrices) e é importante aprender a lidar com este tipo de dado de forma eficiente para reduzir o consumo de memória.\n",
        "\n",
        "Ler mais em: [SkLearn Sparsity](http://scikit-learn.org/stable/modules/feature_extraction.html#sparsity)\n",
        "\n",
        "\n",
        "#### Frequency Encoding\n",
        "\n",
        "Podemos mapear os dados de acordo com a frequencia que eles aparecem na base de dados. Por exemplo, se um determinado valor (C) está em 30% da base, 50% do valor (S) está na base e 20% dos valores são (Q) então teremos:\n",
        "\n",
        "* [S, C, Q] -> [0.50, 0.30, 0.20]\n",
        "\n",
        "\n",
        "Este tipo de técnica vai preservar informações sobre a distribuição dos valores e pode **ajudar tanto modelos lineares quanto tree-models**\n",
        "\n",
        "```\n",
        "encoding = titanic.groupby('Embarked').size()\n",
        "encoding = encoding / len(titanic)\n",
        "titanic['enc'] = titanic.Embarked.map(enconding)\n",
        "```\n",
        "\n",
        "#### Unindo features categoricas\n",
        "\n",
        "Quando existe uma correlação entre duas ou mais features com o target, é interessante criar os dummies da união das duas features: \n",
        "Exemplo:\n",
        "\n",
        "```\n",
        "pclass = [3, 1, 3, 1]\n",
        "sex = ['male', 'female', 'female', 'female']\n",
        "\n",
        "# Basta criar uma outra feature concatenando o texto das duas features juntas\n",
        "pclass_sex = ['male3', '1female', '3female', '1female']\n",
        "```\n",
        "\n",
        "Agora executar o get_dummies e teremos a relação entre as duas categorias.\n",
        "\n",
        "\n",
        "### Conclusão:\n",
        "\n",
        "1. Values in ordinal features are sorted in some meaningful order\n",
        "2. Label encoding maps categories to numbers\n",
        "3. Frequency encoding maps categories to their frequencies\n",
        "4. label and Frequency encodings are often used for tree-based models\n",
        "5. One-hot encoding is often used for non-tree-based models\n",
        "6. Interactions of categorical features can he,help linear models and kNN\n"
      ]
    },
    {
      "metadata": {
        "id": "l_mN8_4EjnFH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Datetime\n",
        "\n",
        "Este tipo de informação pode ser dividida em duas categorias, uma data especifica de um evento e quanto tempo se passou a partir de um evento.\n",
        "\n",
        "1. Features de periodicidade podemos usar informações como:\n",
        "    * Número do dia na semana\n",
        "    * Mes\n",
        "    * Estação do ano (season)\n",
        "    * Ano\n",
        "    * Segundo\n",
        "    * Minuto\n",
        "    * Hora\n",
        "\n",
        "   Estes tipos de informações são úteis para identificar padrões repetitivos dos dados.\n",
        "\n",
        "2. Time Since\n",
        "    * Row-independent moment (Ex.: since 00:00:00 UTC, 1 January 1970)\n",
        "    * Row-dependent important moment\n",
        "        * Número de dias restantes até um feriado próximo / quanto tempo se passou após o último feriado.\n",
        "\n",
        "3. Diferença entre datas\n",
        "\n",
        "## Coordinates\n",
        "\n",
        "* Calcular a distancia de um ponto com pontos importantes. Ex.:\n",
        "   * Calcular a distancia de um ponto importante na cidade (centro da cidade, alguma construção importante)\n",
        "   * Calcular um cluster e pegar a distancia do centro do cluster\n",
        "   * Calcular informações estatisticas a partir de um determinado ponto. Ex.: \n",
        "      * número de flats disponíveis em um determinado ponto\n",
        "      * identificar qual o custo por metro quadrado de uma determinada área\n",
        "\n",
        "Você também pode adicionar  coordenadas \"slightly rotated\" como novas features."
      ]
    },
    {
      "metadata": {
        "id": "rWucGosJkJq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Handling missing values\n",
        "\n",
        "Missing values podem ser NaN, strings vazias e também outliers como -999 ou 999.\n",
        "\n",
        "Uma forma de entender melhor os missing values é plotar um histogram.\n",
        "\n",
        "As principais formas de resolver questões com NaN são:\n",
        "* Substituir NaN com valores -999, -1, etc\n",
        "   * Este tipo de método é interessante especialmente em tree-models onde ele consegue separar os dados em categorias diferentes, facilitando o processo\n",
        "   * Modelos lineares tendem a não responder muito bem a esse tipo de dado\n",
        "* Calcular a média ou mediana\n",
        "   * Este método é bom para non-tree-models mas pode ser confuso para tree-models identificarem que existem dados faltantes\n",
        "      * Uma saída é gerar uma nova features indicando quais as linhas tiveram NaN para uma determinada feature, isto irá melhorar a performance tanto em tree-models quanto non-tree-models\n",
        "* Reconstruir o valor\n",
        "   * É possível reconstruir valores em uma série de dados\n",
        "\n",
        "XgBoost já considera dados NaN, é importante tomar cuidado para que ao reconstruir os dados NaN ou criar novas features a partir destes dados não prejudicar a performance deste tipo de modelo.\n",
        "\n",
        "### Conclusão:\n",
        "\n",
        "1. A escolha do método para tratar os NaNs depende da situação\n",
        "2. Uma forma normal de tratar os NaNs é substituir os mesmos por -999, mean and median\n",
        "3. Dados faltantes podem ser substituidos por alguma outra coisa pelos criadores do dataset\n",
        "4. Criar uma feature \"isnull\" pode ser benéfico\n",
        "5. Em geral, evite preencher os dados com NaNs antes da geração de features\n",
        "6. Xgboost trata NaNs"
      ]
    },
    {
      "metadata": {
        "id": "tqWs4COIkYHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mean encoding\n",
        "\n",
        "É uma técnica para gerar features a partir do target. Exemplo:\n",
        "Inserir a informação indicando qual a porcentagem de vezes que uma determinada categoria é 0 e qual a porcentagem de vezes ela é 1.\n",
        "\n",
        "1. Label encoding não traz qualquer correlação com o target\n",
        "2. Mean encoding ajuda a separar os dados a partir do target\n",
        "\n",
        "É importante que o Mean Encoding não pode ser utilizado diretamente e é necessária uma regularização dos dados corretamente. Caso o processo de regularization não seja feito apropriadamente, podemos ter um leak e gerar um overfiting dos dados.\n",
        "\n",
        "### Regularization\n",
        "\n",
        "Existem 4 métodos de Regularization que podemos utilizar:\n",
        "\n",
        "1. CV loop inside training data (recomendado)\n",
        "   * Método bastante robusto e intuitivo\n",
        "   * Normalmente 4 ou 5 folds são o suficiente\n",
        "   * É importante tomar cuidado em situações extremas como Leave-one-out\n",
        "   \n",
        "   ```\n",
        "   y_tr = df_tr['target'].values\n",
        "   skf = StratifiedKFold(y_tr, 5, shuffle=True, random_state=0)\n",
        "   \n",
        "   for tr_ind, val_ind in skf:\n",
        "       X_tr, X_val = df_tr.iloc[tr_ind], df_tr.iloc[val_ind]\n",
        "       for col in cols:\n",
        "           means = X_val[col].map(X_tr.groupby(col).target.mean())\n",
        "           X_val[col+'_mean_target'] = means\n",
        "       train_new.iloc[val_ind] = X_val\n",
        "\n",
        "   prior = df_tr['target'].mean()\n",
        "   train_new.fillna(prior, inplace=True)\n",
        "   ```\n",
        "2. Smoothing\n",
        "    * Se a categoria é grande, com diversas linhas podemos considerar um valor estimado mas se a categoria tem poucos dados (é rara) não podemos confiar.\n",
        "    \n",
        "    ```\n",
        "    # Hyperparam alpha controla o \"amount of regularization\", se ele for 0 não temos regularização e quando ele se aproxima do infinito tudo tende a seguir para o globalmean\n",
        "    row = (mean(target) * nrows + globalmean * alpha) / (nrows + alpha)\n",
        "    ```\n",
        "\n",
        "    * Tudo o que considera \"punir\" uma categoria mais rara, pode ser considerado Smoothing\n",
        "\n",
        "3. Adding random noise\n",
        "    * Este método é bastante instável e é difícil fazer ele funcionar, o principal problema é a quantidade de \"noise\" que devemos adicionar, muito noise irá transformar a feature em lixo e muito pouco não irá ser efetivo.\n",
        "\n",
        "4. Sorting and calculating expanding mean (recomendado)\n",
        "    * Menor quantidade de leakage\n",
        "    * Não existem parametros para tunar\n",
        "    * Qualidade de codificação irregular (irregular encoding quality)\n",
        "    * Built-in CatBoost\n",
        "    \n",
        "    ```\n",
        "    cumsum = df_tr.groupby(col)['target'].cumsum() - df_tr['target']\n",
        "    cumcnt = df_tr.groupby(col).cumcount()\n",
        "    train_new[col + '_mean_target'] = cumsum / cumcnt\n",
        "    ```\n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "O7uLqTceki6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Statistics and distance based features"
      ]
    },
    {
      "metadata": {
        "id": "XrYUbDXrklHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfcw9fW_klkM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Matrix factorizations"
      ]
    },
    {
      "metadata": {
        "id": "G0QyeNSAknV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AO_OVB-6knzD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature interactions"
      ]
    },
    {
      "metadata": {
        "id": "VjCaya3lkpB4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6d-pDR3PkpbC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## t-SNE"
      ]
    },
    {
      "metadata": {
        "id": "jQxYIRcjkqug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOYwIZttkrNq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KNN Features Implementation"
      ]
    },
    {
      "metadata": {
        "id": "WNs71VsHku2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}