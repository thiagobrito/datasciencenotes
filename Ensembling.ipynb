{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensembling",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/thiagobrito/datasciencenotes/blob/master/Ensembling.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "G3FBCyMzOLu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ensemble Methods\n",
        "\n",
        "## Bagging\n",
        "\n",
        "> *Means averaging slightly different versions of the same model to improve accuracy*\n",
        "\n",
        "### Parameters that control bagging\n",
        "\n",
        "* Changing the seed\n",
        "* Row(sub) sampling or bootstrapping\n",
        "* Shuffling\n",
        "* Column (Sub) sampling\n",
        "* Model-specific parameters\n",
        "* Number of models (or bags)\n",
        "* (Optionally) Paralelism\n",
        "\n",
        "### Examples of bagging\n",
        "\n",
        "> BaggingClassifier and BaggingRegressor from SkLearn\n",
        "\n",
        "```\n",
        "# train is the training data\n",
        "# test is the test data\n",
        "# y is the target variable\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "bags=10\n",
        "seed=1\n",
        "bagged_prediction=np.zeros(test.shape[0])\n",
        "for n in range(0, bags):\n",
        "    model.set_params(random_state=seed + n)\n",
        "    model.fit(train, y)\n",
        "    preds = model.predict(test)\n",
        "    bagged_prediction += preds\n",
        "# Take average of predicts\n",
        "bagged_prediction /= bags\n",
        "\n",
        "```\n",
        "\n",
        "## Boosting\n",
        "\n",
        "> A form of weighted averaging of models where each model is built sequentially via taking into account the past model performance.\n",
        "\n",
        "### Weight based\n",
        "\n",
        "[Adaboost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
        "\n",
        "### Residual based\n",
        "\n",
        "* XGBoost\n",
        "* Lightgbm\n",
        "* [H20's GBM](https://github.com/h2oai/h2o-3)\n",
        "* Catboost\n",
        "* Sklearns GBM\n",
        "\n",
        "## Stacking\n",
        "\n",
        "> Means making predictions of a number of models in a hold-out set and then using a different (Meta) model to train on these predictions.\n",
        "\n",
        "### Methodology\n",
        "1. Splitting the train set into two disjoint sets\n",
        "2. Train several base leaners on the first part\n",
        "3. Make predictions with the base leaners on the second (validation) part\n",
        "4. Using the predictions from (3) as the input to train a higher level learner\n",
        "\n",
        "```\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "#train is the training data\n",
        "# test is the test data\n",
        "# y is the target variable for the train data\n",
        "\n",
        "training,valid,ytraining,yvalid = train_test_split(train, y, test_size=0.5)\n",
        "\n",
        "model1 = RandomForestRegressor()\n",
        "model2 = LinearRegression()\n",
        "\n",
        "model1.fit(training, ytraining)\n",
        "model2.fit(training, ytraining)\n",
        "\n",
        "preds1 = model1.predict(valid)\n",
        "preds2 = model2.predict(valid)\n",
        "\n",
        "test_preds1 = model1.predict(test)\n",
        "test_preds2 = model2.predict(test)\n",
        "\n",
        "stacked_predictions = np.column_stack((preds1, preds2))\n",
        "stacked_test_predictions = np.column_stack((test_preds1, test_preds2))\n",
        "\n",
        "meta_model = LinearRegression()\n",
        "meta_model.fit(stacked_predictions, yvalid)\n",
        "\n",
        "final_predictions = meta_model.predict(stacked_test_predictions)\n",
        "```\n",
        "\n",
        "### Pay attention\n",
        "* With time sensitive data - respect time\n",
        "* Diversity as important as performance (how different is one model from the other)\n",
        "* Diversity may come from:\n",
        "   * Different algoritms\n",
        "   * Different input features\n",
        "* Performance plateauing after N models\n",
        "* Meta model is normally modest\n",
        "\n",
        "## StackNet\n",
        "\n",
        "> A scalable meta modelling methodology that utilizes stacking to combine multiple models in a neural network archtecture of multiple levels.\n",
        "\n",
        "### StackNet as a neural network\n",
        "* In a neural network, every node is a simple linear model (like linear regression) with some non linear transformation.\n",
        "* Instead of a linear model we could use any model\n",
        "\n",
        "### How to train\n",
        "* We cannot use Backpropagation (not all models are differentiable)\n",
        "* We use stacking to link each model/node with target\n",
        "* Use K-Fold paradigm (to extend to many levels, we can use a Kfold paradigm)\n",
        "* No epochs - different connections instead\n",
        "\n",
        "\n",
        "## Ensembling Tips and Tricks\n",
        "\n",
        "### Level 1\n",
        "* Diversity based on algorithms\n",
        "   * 2-3 gradient boosted trees (lightgb, xgboost, H2O, catboost)\n",
        "   * 2-3 neural nets (keras, pytorch) (one with 1 hidden-layer, another with 2 and another with 3)\n",
        "   * 1-2 ExtraTrees/Random Forest (sklearn)\n",
        "   * 1-2 knn models (sklearn)\n",
        "   * 1 Factorization machine (libfm)\n",
        "   * 1 SVM with nonlinear kernel if size/memory allows (sklearn)\n",
        "   \n",
        "* Diversity based on input data:\n",
        "   * Categorical features: One hot, label encoding, target encoding\n",
        "   * Numerical features: outliers, binning derivatives, percentiles, scaling\n",
        "   * Interactions: col1*/+-col2, groupby, unsupervised\n",
        "   \n",
        "### Subsequent level tips\n",
        "* Simpler (or shallower) algoritms:\n",
        "   * gradient boosted trees with small depth (like 2 or 3)\n",
        "   * Linear models with high regularization\n",
        "   * ExtraTrees\n",
        "   * Shallow networks (as in 1 hidden layer)\n",
        "   * knn with BrayCurtis Distance\n",
        "   * Brute forcing a search for best linear weights based on cv\n",
        "\n",
        "### Feature engineering:\n",
        "* pairwise differences between meta features\n",
        "* row-wise statistics like averages or stds\n",
        "* Standard feature selection techniques\n",
        "* For every 7.5 models in previous level we add 1 in meta (subsequent layer)\n",
        "* Be mindful or target leakage (control the k-folds number keep small)\n",
        "\n",
        "### Software for stacking\n",
        "* StackNet (https://github.com/kaz-anova/StackNet)\n",
        "* Stacked ensembles from H2O\n",
        "* Xcessiv (https://github.com/reiinakano/xcessiv)\n",
        "\n",
        "### Links interessantes:\n",
        "[Parametros importantes dos modelos](https://github.com/kaz-Anova/StackNet/blob/master/parameters/PARAMETERS.MD)\n",
        "\n",
        "\n",
        "# Other ideas\n",
        "\n",
        "There are a number of ways to validate second level models (meta-models). In this reading material you will find a description for the most popular ones. If not specified, we assume that the data does not have a time component. We also assume we already validated and fixed hyperparameters for the first level models (models).\n",
        "\n",
        "### Simple holdout scheme\n",
        "Split train data into three parts: partA and partB and partC.\n",
        "Fit N diverse models on partA, predict for partB, partC, test_data getting meta-features partB_meta, partC_meta and test_meta respectively.\n",
        "Fit a metamodel to a partB_meta while validating its hyperparameters on partC_meta.\n",
        "When the metamodel is validated, fit it to [partB_meta, partC_meta] and predict for test_meta.\n",
        "\n",
        "### Meta holdout scheme with OOF meta-features\n",
        "\n",
        "Split train data into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in train_data we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them train_meta.\n",
        "Fit models to whole train data and predict for test data. Let's call these features test_meta.\n",
        "Split train_meta into two parts: train_metaA and train_metaB. Fit a meta-model to train_metaA while validating its hyperparameters on train_metaB.\n",
        "When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
        "\n",
        "### Meta KFold scheme with OOF meta-features\n",
        "\n",
        "Obtain OOF predictions train_meta and test metafeatures test_meta using b.1 and b.2.\n",
        "Use KFold scheme on train_meta to validate hyperparameters for meta-model. A common practice to fix seed for this KFold to be the same as seed for KFold used to get OOF predictions.\n",
        "When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
        "\n",
        "### Holdout scheme with OOF meta-features\n",
        "\n",
        "Split train data into two parts: partA and partB.\n",
        "Split partA into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in partA we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them partA_meta.\n",
        "Fit models to whole partA and predict for partB and test_data, getting partB_meta and test_meta respectively.\n",
        "Fit a meta-model to a partA_meta, using partB_meta to validate its hyperparameters.\n",
        "When the meta-model is validated basically do 2. and 3. without dividing train_data into parts and then train a meta-model. That is, first get out-of-fold predictions train_meta for the train_data using models. Then train models on train_data, predict for test_data, getting test_meta. Train meta-model on the train_meta and predict for test_meta.\n",
        "\n",
        "### KFold scheme with OOF meta-features\n",
        "\n",
        "To validate the model we basically do d.1 -- d.4 but we divide train data into parts partA and partB M times using KFold strategy with M folds.\n",
        "When the meta-model is validated do d.5.\n",
        "\n",
        "Validation in presence of time component\n",
        "\n",
        "### KFold scheme in time series\n",
        "\n",
        "In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n",
        "\n",
        "Split the train data into chunks of duration T. Select first M chunks.\n",
        "Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
        "Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features.\n",
        "g) KFold scheme in time series with limited amount of data\n",
        "\n",
        "We may often encounter a situation, where scheme f) is not applicable, especially with limited amount of data. For example, when we have only years 2014, 2015, 2016 in train and we need to predict for a whole year 2017 in test. In such cases scheme c) could be of help, but with one constraint: KFold split should be done with the respect to the time component. For example, in case of data with several years we would treat each year as a fold."
      ]
    },
    {
      "metadata": {
        "id": "Xcvh3jy_OLRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}